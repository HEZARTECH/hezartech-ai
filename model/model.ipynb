{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce3c9365-6cad-4292-90f6-91c38c0101bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391fa65f-cd6a-44e8-80b6-63a68438e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output v6.csv')\n",
    "\n",
    "label_mapping = {\n",
    "    None: 0,\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 2,\n",
    "    \"positive|negative\": 3,\n",
    "}\n",
    "\n",
    "# Etiketleri güncelle\n",
    "df['label'] = df['value'].map({\n",
    "    1: 'positive',\n",
    "    2: 'negative',\n",
    "    3: 'positive|negative',\n",
    "})\n",
    "\n",
    "# Sadece gerekli sütunları tutun\n",
    "df = df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cda2a44-814e-4b3b-acd6-76afa42b49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa6f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-128k-uncased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04d3f2af-b672-40c1-b5e0-667e15dbc6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(128000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"dbmdz/bert-base-turkish-128k-uncased\",\n",
    "    num_labels = len(label_mapping),\n",
    "    # output_attentions = False,\n",
    "    # output_hidden_states = False,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "NOTE:When we are retrain our model we should use that. (Un-comment block is should work and enough.)\n",
    "\n",
    "# Load the state dict\n",
    "state_dict = torch.load('./results/checkpoint-final/pytorch_model.bin', map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\"\"\"\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04f9d1e4-48f5-4581-9225-dd85c90cbc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Dataset oluşturma\n",
    "X_train = X_train.astype(str)\n",
    "X_test = X_test.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8120cc3d-18bc-4d77-a33d-4d4799c8b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13244    Telefon faturamı incelediğimde iletişim ücretl...\n",
      "5421     Türk Telekom alt yapısındaki Pttcell müşterisi...\n",
      "15258                              Rezaletin bini bir para\n",
      "29066                                     yönlendiriyorlar\n",
      "28667                              asansörde mahsur kaldık\n",
      "Name: text, dtype: object\n",
      "13066    #vakifbank kredi başvurum hala sonuçlanmadı. A...\n",
      "27521                                boş yere açık kalıyor\n",
      "3842                               beklediğimden çok geldi\n",
      "2767                                    borcunuz yok diyor\n",
      "19632    Turk Telekom ev adresimi ev ve cep telefonu bi...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5fa45ce-b22f-42aa-a245-1d19b0a6d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_train, 'label': y_train}))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_test, 'label': y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba127871-08f2-4899-80e2-f02adcd1e365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824e02e306894857bb541a7a35b281ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0531940db5748bda8b056d9e5d061bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply the tokenization function\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17c6d146-be8d-4e96-b55f-c70745953f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fab8a0d3844d4283593fbad4ae537e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8768d8068845168dee0fe5fedd7c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_labels(example):\n",
    "    example['labels'] = label_mapping[example['label']]\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(add_labels)\n",
    "test_dataset = test_dataset.map(add_labels)\n",
    "\n",
    "# Remove the 'label' column since we now have 'labels'\n",
    "train_dataset = train_dataset.remove_columns(['label'])\n",
    "test_dataset = test_dataset.remove_columns(['label'])\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5e3a8ab-edec-4191-ac00-e7fb2b3731b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4438560b-83ea-40ed-a3b9-f743c0180010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f2779f67864349b91eeab04010bb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3427, 'grad_norm': 0.9962624311447144, 'learning_rate': 4.9945498146937e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8495, 'grad_norm': 11.76801872253418, 'learning_rate': 4.99018966644866e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Resume training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# trainer.train(resume_from_checkpoint='./results/checkpoint-final)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Resume training\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint='./results/checkpoint-final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b0810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and optimizer state with contiguous tensors\n",
    "def save_model_with_contiguous_tensors(model, output_dir):\n",
    "    state_dict = model.state_dict()\n",
    "    contiguous_state_dict = {k: v.contiguous().to('cpu') for k, v in state_dict.items()}\n",
    "    torch.save(contiguous_state_dict, f\"{output_dir}/pytorch_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and optimizer state\n",
    "save_model_with_contiguous_tensors(model, './results/checkpoint-final')\n",
    "trainer.state.save_to_json('./results/checkpoint-final/trainer_state.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb0d02-20d1-4b07-ab96-933bfd580c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9494b-ac54-47a5-a78b-ed0d0fbc98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accuracy metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions_labels = predictions.predictions.argmax(-1)\n",
    "accuracy = accuracy_metric.compute(predictions=predictions_labels, references=predictions.label_ids)\n",
    "print(f\"Accuracy: {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c94b2c-c9ce-41ac-81c6-f882fb6a69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load F1 Metric\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_metric.compute(predictions=predictions_labels, references=predictions.label_ids, average='weighted')\n",
    "print(f\"F1 Score: {f1['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f235dca-99cc-4890-8ae9-2de7baa5853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'w') as file:\n",
    "    file.write(f'Accuracy: {accuracy[\"accuracy\"]}\\n')\n",
    "    file.write(f'F1 Score: {f1[\"f1\"]}\\n')\n",
    "    file.write(f'Evaluation results: {eval_results}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
